<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Stamatis Alexandropoulos</title>

    <meta name="author" content="Stamatis Alexandropoulos">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Stamatis Alexandropoulos
                </p>
                <p>I am a 3rd year Ph.D student in Computer Science at <a href="https://www.princeton.edu/">Princeton University</a>, working with <a href="https://www.cs.princeton.edu/~jiadeng/">Prof. Jia Deng</a>  as part of the <a href="https://pvl.cs.princeton.edu/">Princeton Vision & Learning Lab</a>. Previously, I completed with highest honors (Valedictorian) my Diploma M.Eng Degree at the <a href="https://www.ece.ntua.gr/en">School of Electrical and Computer Engineering</a> of the <a href="https://www.ntua.gr/en/">National Technical University of Athens</a>, where I worked with <a href="http://cvsp.cs.ntua.gr/maragos/">Prof. Petros Maragos</a>  and <a href="http://people.ee.ethz.ch/~csakarid/"> Dr. Christos Sakaridis</a> (<a href="https://vision.ee.ethz.ch/">Computer Vision Lab</a>, <a href="https://ethz.ch/en.html">ETH Zurich</a>).
                </p>
                <p>
                  My research interests lie in the interface of Machine Learning, Computer Vision, Computer Graphics as well as Robotics. Consequently, I would like to delve into the above areas and develop artificially intelligent systems able to make an impact on humanity and reason visual world.
                </p>
                <p style="text-align:center">
                  <a href="mailto:sa6924@princeton.edu">Email</a> &nbsp;/&nbsp;
                  <!-- <a href="data/JonBarron-CV.pdf">CV</a> &nbsp;/&nbsp; -->
                  <!-- <a href="https://scholar.google.com/citations?user=Yi_ZstgAAAAJ&hl=el&authuser=1">Google Scholar</a> &nbsp;/&nbsp; -->
                  <a href="https://github.com/stamatisalex">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/photo_site.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/photo_site.png"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>News</h2>
                <p>
                  &#x2022; [Sep 2025] We got a paper accepted in <b>NeurIPS 2025</b>: "InFlux: A Benchmark for Self-Calibration of Dynamic Intrinsics of Video Cameras"
                </p>
                <p>
                  &#x2022; [Jun 2025] We got a paper accepted in <b>ICCV 2025</b>: "Princeton365: A Diverse Dataset with Accurate Camera Pose"
                </p>
                <p>
                  &#x2022; [May 2025] We got a paper in arxiv: "Infinigen-Sim: Procedural Generation of Articulated Simulation Assets"
                </p>
                <p>
                  &#x2022; [Mar 2024] We got a paper accepted in <b>CVPR 2024</b>: "Infinigen Indoors: Photorealistic Indoor Scenes using Procedural Generation"
                </p>
                <p>
                  &#x2022; [Oct 2023] We got a paper accepted in <b>IEEE/CVF WACV 2024</b>: "OVeNet: Offset Vector Network for Semantic Segmentation"
                </p>
                <p>
                  &#x2022; [Sep 2023] I have been awarded the Onassis Foundation Scholarship for my graduate studies!
                </p>
                <p>
                  &#x2022; [Aug 2023] I have been awarded the Princeton Stanley J. Seeger Hellenic Studies Award 
                </p>
                <p>
                  &#x2022; [Aug 2023] I am starting my Ph.D at Princeton in Fall 2023, advised by Professor Jia Deng! 
                </p>
                <p>
                  &#x2022; [June 2023] I have been awarded the Tzafestas, Chrisovergi and Kondouli Honorary Awards for graduating <b>1st in class among all students of the Department of ECE</b> (Valedictorian) and <b>across all Departments of National Technical University of Athens</b>, respectively.
                </p>
                <p>
                  &#x2022; [June 2023] I have been awarded the Kontaxis Honors for graduating 1st in Computer Science major
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          
    

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/princeton365.png" alt="clean-usnob" width="180" height="120">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://princeton365.cs.princeton.edu/">
                  <span class="papertitle"  style="text-decoration: underline; "> Princeton365: A Diverse Dataset with Accurate Camera Pose </span>
                </a>
                <br>
                 <b> Stamatis Alexandropoulos *</b>,
                Karhan Kayan*,
                Rishabh Jain,
                Yiming Zuo,
                Erich Liang,
                Jia Deng</a>,                
                <br>
                (*equal contribution)
                <br>
                <em>Internation Conference of Computer Vision (ICCV)</em>, 2025
                <br>
                <a href="https://princeton365.cs.princeton.edu/">[Website]</a>
                <a href="https://arxiv.org/abs/2506.09035">[Pdf]</a>
                  
                <a href="https://github.com/princeton-vl/Princeton365">[Code]</a>
                
                <p></p>
                <p>
                  We introduce Princeton365, a large-scale diverse dataset of 365 videos with accurate camera pose.
                </p>
              </td>
            </tr>




            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/influx.png" alt="clean-usnob" width="180" height="120">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://influx.cs.princeton.edu/">
                  <span class="papertitle"  style="text-decoration: underline; "> Princeton365: A Diverse Dataset with Accurate Camera Pose </span>
                </a>
                <br>
                Erich Liang, Roma Bhattacharjee*, Sreemanti Dey*, Rafael Moschopoulos, Caitlin Wang, Michel Liao, Grace Tan, Andrew Wang, Karhan Kayan, Stamatis Alexandropoulos, Jia Deng</a>,                
                <br>
                (*equal contribution)
                <br>
                <em>Neural Information Processing Systems Datasets and Benchmarks Track (NeurIPS), 2025</em>, 2025
                <br>
                <a href="https://influx.cs.princeton.edu/">[Website]</a>
                <a href="https://arxiv.org/abs/2510.23589">[Pdf]</a>
                  
                <a href="https://github.com/princeton-vl/InFlux/tree/main">[Code]</a>
                
                <p></p>
                <p>
                  We present Intrinsics in Flux (InFlux), a real-world benchmark that provides per-frame ground truth intrinsics annotations for videos with dynamic intrinsics                </p>
              </td>
            </tr>



            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/infinigen_sim.png" alt="clean-usnob" width="180" height="120">
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2505.10755">
                  <span class="papertitle"  style="text-decoration: underline; "> Infinigen-Sim: Procedural Generation of Articulated Simulation Assets </span>
                </a>
                <br>
                Abhishek Joshi, Beining Han, Jack Nugent, Yiming Zuo, Jonathan Liu, Hongyu Wen, <b>Stamatis Alexandropoulos </b>, Tao Sun, Alexander Raistrick, Gaowen Liu, Yi Shao, Jia Deng
                <br>
                <em>arxiv</em>, 2025
                <br>
                <a href="https://arxiv.org/abs/2505.10755">[Pdf]</a>
                                  
                <p></p>
                <p>
                  We introduce Princeton365, a large-scale diverse dataset of 365 videos with accurate camera pose.
                </p>
              </td>
            </tr>





      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="images/infinigen.png" alt="clean-usnob" width="180" height="120">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://arxiv.org/abs/2406.11824">
            <span class="papertitle"  style="text-decoration: underline; "> Infinigen Indoors: Photorealistic Indoor Scenes using Procedural Generation</span>
          </a>
          <br>
          Alexander Raistrick*, Karhan Kayan*, Lingjie Mei*, David Yan, Yiming Zuo, Beining Han, Hongyu Wen, Meenal Parakh, <b>Stamatis Alexandropoulos</b>, Lahav Lipson, Zeyu Ma, Jia Deng
          <br>
          (*equal contribution)
          <br>
          <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2024
          <br>
          <a href="https://infinigen.org/">[Website]</a>
          <a href="https://arxiv.org/abs/2406.11824">[Pdf]</a>
            
          <a href="https://github.com/princeton-vl/infinigen">[Code]</a>
          
          <!-- <a href="https://arxiv.org/abs/2303.14516">[arXiv]</a> -->
          <p></p>
          <p>
            We introduce Infinigen Indoors, a Blender-based procedural generator of photorealistic indoor scenes.
          </p>
        </td>
      </tr>
      
      <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
          <img src="images/ovenet.png" alt="clean-usnob" width="180" height="140">
        </td>
        <td style="padding:20px;width:75%;vertical-align:middle">
          <a href="https://arxiv.org/pdf/2303.14516.pdf">
            <span class="papertitle" style="text-decoration: underline;" >OVeNet: Offset Vector Network for Semantic Segmentation</span>
          </a>
          <br>
          <strong>Stamatis Alexandropoulos</strong>,
          Christos Sakaridis,
          Petros Maragos, <br>
          <br>
          <em>IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</em>, 2024
          <br>
          <a href="https://arxiv.org/pdf/2303.14516.pdf">[Pdf]</a>
           
          <a href="https://github.com/stamatisalex/OVeNet">[Code]</a>
          
          <!-- <a href="https://arxiv.org/abs/2303.14516">[arXiv]</a> -->
          <p></p>
          <p>
            Based on knowledge about the high regularity of real-world scenes, we propose a method for improving class predictions by learning to selectively exploit information from neighboring pixels.
          </p>
        </td>
      </tr>


    
<!-- 
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/bd_promo.jpg" alt="blind-date" width="160" height="160">
              </td>
              <td width="75%" valign="middle">
                <a href="https://drive.google.com/file/d/1PQjzKgFcrAesMIDJr-WDlCwuGUxZJZwO/view?usp=sharing">
                  <span class="papertitle">Blind Date: Using Proper Motions to Determine the Ages of Historical Images</span>
                </a>
                <br>
                <strong>Jonathan T. Barron</strong>, <a href="http://cosmo.nyu.edu/hogg/">David W. Hogg</a>, <a href="http://www.astro.princeton.edu/~dstn/">Dustin Lang</a>, <a href="http://cs.nyu.edu/~roweis/">Sam Roweis</a>
                <br>
                <em>The Astronomical Journal</em>, 136, 2008
                <p>Using the relative motions of stars we can accurately estimate the date of origin of historical astronomical images.</p>
              </td>
            </tr>

            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/clean_promo.jpg" alt="clean-usnob" width="160" height="160">
              </td>
              <td width="75%" valign="middle">
                <a href="https://drive.google.com/file/d/1YvRx-4hrZoCk-nl6OgVJZlHAqOiN5hWq/view?usp=sharing">
                  <span class="papertitle">Cleaning the USNO-B Catalog Through Automatic Detection of Optical Artifacts</span>
                </a>
                <br>
                <strong>Jonathan T. Barron</strong>, <a href="http://stumm.ca/">Christopher Stumm</a>, <a href="http://cosmo.nyu.edu/hogg/">David W. Hogg</a>, <a href="http://www.astro.princeton.edu/~dstn/">Dustin Lang</a>, <a href="http://cs.nyu.edu/~roweis/">Sam Roweis</a>
                <br>
                <em>The Astronomical Journal</em>, 135, 2008
                <p>We use computer vision techniques to identify and remove diffraction spikes and reflection halos in the USNO-B Catalog.</p>
                <p>In use at <a href="http://www.astrometry.net">Astrometry.net</a></p>
              </td>
            </tr> -->

          </tbody></table>

          
          <!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Miscellanea</h2>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cvf.jpg"></td>
              <td width="75%" valign="center">
                <a href="https://cvpr2023.thecvf.com/Conferences/2023/Organizers">Demo Chair, CVPR 2023</a>
          <br>
                <a href="https://cvpr2022.thecvf.com/area-chairs">Area Chair, CVPR 2022</a>
                <br>
                <a href="http://cvpr2021.thecvf.com/area-chairs">Area Chair & Longuet-Higgins Award Committee Member, CVPR 2021</a>
                <br>
                <a href="http://cvpr2019.thecvf.com/area_chairs">Area Chair, CVPR 2019</a>
                <br>
                <a href="http://cvpr2018.thecvf.com/organizers/area_chairs">Area Chair, CVPR 2018</a>
              </td>
            </tr>
            <tr>
              <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/cs188.jpg" alt="cs188">
              </td>
              <td width="75%" valign="center">
                <a href="http://inst.eecs.berkeley.edu/~cs188/sp11/announcements.html">Graduate Student Instructor, CS188 Spring 2011</a>
                <br>
                <a href="http://inst.eecs.berkeley.edu/~cs188/fa10/announcements.html">Graduate Student Instructor, CS188 Fall 2010</a>
                <br>
                <a href="http://aima.cs.berkeley.edu/">Figures, "Artificial Intelligence: A Modern Approach", 3rd Edition</a>
              </td>
            </tr>
            

            <tr>
              <td align="center" style="padding:20px;width:25%;vertical-align:middle">
                <h2>Basically <br> Blog Posts</h2>
              </td>
              <td width="75%" valign="middle">
                <a href="https://arxiv.org/abs/2112.11687">Squareplus: A Softplus-Like Algebraic Rectifier</a>
                <br>
                <a href="https://arxiv.org/abs/2010.09714">A Convenient Generalization of Schlick's Bias and Gain Functions</a>
                <br>
                <a href="https://arxiv.org/abs/1704.07483">Continuously Differentiable Exponential Linear Units</a>
                <br>
                <a href="https://jonbarron.info/data/cvpr2023_llm_workshop_annotated.pdf">Scholars & Big Models: How Can Academics Adapt?</a>
              </td>
            </tr>
             -->
            
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Design and source code from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron's website</a>
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
